# CutAndQC performs initial QC on CutAndTag projects
import glob
import os
import sys
from pathlib import Path, PurePath, PurePosixPath
from collections import defaultdict
import pandas as pd
from peppy import Project
from snakemake.utils import validate, min_version
#import plotly as plt
#import plotly.graph_objects as go

##### set minimum snakemake version #####
min_version("8.20.1")

include: "src/common.py"


def _get_nested(mapping, *keys, default=None):
    """
    Safely retrieve nested config values.
    """
    node = mapping
    for key in keys:
        if not isinstance(node, dict):
            return default
        node = node.get(key)
        if node is None:
            return default
    return node


def _hydrate_config(cfg):
    """
    Populate legacy top-level keys from the new hierarchical config layout so
    the existing rules continue to work without large refactors.
    """
    project = _get_nested(cfg, "project_info", default={})
    samples = _get_nested(project, "samples", default={})
    outputs = _get_nested(project, "outputs", default={})
    refs = _get_nested(cfg, "reference", default={})
    steps = _get_nested(cfg, "pipeline_steps", default={})
    peaks = _get_nested(steps, "peaks", default={})
    preprocessing = _get_nested(steps, "preprocessing", default={})
    diffbind = _get_nested(steps, "diffbind", default={})
    workflow_res = _get_nested(cfg, "workflow_resources", default={})
    exec_cfg = _get_nested(workflow_res, "execution", default={})

    cfg["config_file"] = samples.get("table", "")
    cfg["pep_config_file"] = samples.get("pep_config", "")
    cfg["pep_attributes"] = samples.get("pep_attributes", {})
    cfg["output_base_dir"] = outputs.get("root", "")
    cfg["output_structure"] = outputs.get("structure", {})

    cfg["USEIGG"] = peaks.get("use_igg", False)
    cfg["IGG"] = peaks.get("igg_token", "IgG")
    cfg["N_INTERSECTS"] = peaks.get("n_intersects", 2)
    cfg["TRIM_ADAPTERS"] = preprocessing.get("trim_adapters", False)
    cfg["padj_cutoff"] = diffbind.get("padj_cutoff", 0.05)

    cfg["USE_SINGULARITY"] = exec_cfg.get("use_singularity", False)
    cfg["SINGULARITY_IMAGE_FOLDER"] = exec_cfg.get("singularity_image_folder", "")

    cfg["GENOME"] = refs.get("genome_index", "")
    cfg["FASTA"] = refs.get("genome_fasta", "")
    cfg["GENES"] = refs.get("genes_bed", "")
    cfg["CSIZES"] = refs.get("chrom_sizes", "")
    cfg["ADAPTER_FASTA"] = refs.get("adapter_fasta", "")
    cfg["BLACKLIST"] = refs.get("blacklist", "") or ""


configfile: "config/config.yml"
_hydrate_config(config)
validate(config, schema="../schemas/config.schema.yml")


def _resolve_config_paths(node, paths_dict):
    """
    Recursively replace placeholders such as {raw_data} using values defined in
    the PEP `paths` block.
    """
    if not paths_dict:
        return node
    if isinstance(node, dict):
        for key, value in node.items():
            node[key] = _resolve_config_paths(value, paths_dict)
        return node
    if isinstance(node, list):
        return [_resolve_config_paths(item, paths_dict) for item in node]
    if isinstance(node, str) and "{" in node and "}" in node:
        resolved = node
        for placeholder, replacement in paths_dict.items():
            if replacement is None:
                continue
            resolved = resolved.replace(f"{{{placeholder}}}", str(replacement))
        return resolved
    return node


def _explode_list_columns(df):
    """
    Expand list-valued columns that can be produced by PEP modifiers.
    """
    list_cols = [
        col
        for col in df.columns
        if df[col].apply(lambda x: isinstance(x, (list, tuple))).any()
    ]
    if not list_cols:
        return df
    expanded = df.copy()
    for col in list_cols:
        expanded = expanded.explode(col, ignore_index=True)
    return expanded


def _ensure_sample_columns(df):
    """
    Normalize column names expected by downstream logic.
    """
    normalized = df.copy()
    if "sample" not in normalized.columns:
        if "sample_name" in normalized.columns:
            normalized["sample"] = normalized["sample_name"].astype(str)
        elif normalized.index.name:
            normalized["sample"] = normalized.index.astype(str)
        else:
            raise ValueError(
                "PEP sample table is missing a 'sample' or 'sample_name' column."
            )
    normalized["sample"] = normalized["sample"].astype(str)
    if "run" not in normalized.columns:
        normalized["run"] = 1
    normalized["run"] = pd.to_numeric(normalized["run"], errors="raise").astype(int)
    return normalized


# Load (optional) PEP configuration only for path placeholders
pep_config_value = _get_nested(
    config,
    "project_info",
    "samples",
    "pep_config",
    default=os.path.join("config", "pep", "project_config.yaml"),
)
pep_config_path = (
    pep_config_value
    if os.path.isabs(pep_config_value)
    else os.path.join(workflow.basedir, pep_config_value)
)

try:
    pep_project = Project(pep_config_path)
except Exception:
    pep_project = None

if pep_project is not None:
    # merge PEP paths
    if "paths" in pep_project.config:
        config.setdefault("paths", {}).update(pep_project.config["paths"])
    # also expose commonly used project-level paths for placeholder resolution
    proj_cfg = pep_project.config.get("project", {})
    raw_data = proj_cfg.get("raw_data")
    if raw_data:
        config.setdefault("paths", {})["raw_data"] = raw_data
    proj_name = proj_cfg.get("name")
    if proj_name:
        config["paths"]["project_name"] = proj_name

_resolve_config_paths(config, config.get("paths", {}))

# ---------------------------------------------------------------------------
# Build sample annotation table directly from a CSV/TSV file
# ---------------------------------------------------------------------------
sample_table_rel = _get_nested(
    config,
    "project_info",
    "samples",
    "table",
)
if not sample_table_rel:
    raise ValueError(
        "Config is missing 'project_info -> samples -> table' with path to sample sheet."
    )

sample_table_path = (
    sample_table_rel
    if os.path.isabs(sample_table_rel)
    else os.path.join(workflow.basedir, sample_table_rel)
)

# Try to infer separator: default to comma, fall back to tab if only one column
annot = pd.read_csv(sample_table_path, sep=",")
if annot.shape[1] == 1:
    annot = pd.read_csv(sample_table_path, sep="\t")

if annot.empty:
    raise ValueError(f"Sample table at {sample_table_path} is empty.")

# Normalise column names to what the workflow expects
annot = _ensure_sample_columns(annot)

if "read_type" not in annot.columns:
    read_type = _get_nested(config, "project_info", "read_type", default="paired-end")
    annot["read_type"] = read_type

annot["sample_run"] = (
    annot["sample"].astype(str) + "_" + annot["run"].astype(str)
)

# Validate and guard against duplicate sample/run combinations
validate(annot, schema="../schemas/samples.schema.yml")
duplicates = annot[annot.duplicated(subset=["sample_run"], keep=False)]
if not duplicates.empty:
    sys.stderr.write(
        "\nError: Duplicate (sample, run) pairs found in the sample table.\n"
    )
    sys.stderr.write(str(duplicates[["sample", "run", "sample_run"]]) + "\n\n")
    sys.exit(1)

annot["run"] = annot["run"].astype(str)
annot = annot.set_index("sample", drop=False)

SAMPLE_SHEET = os.path.abspath(sample_table_path)
st = annot


#deseq2_md = pd.read_table("config/deseq2_metadata.csv", sep = ",").set_index('sample', drop = False)
#validate(deseq2_md, schema="../schemas/deseq2.schema.yml")

#diffbind_md = pd.read_table("config/diffbind_config.csv", sep = ",").set_index('SampleID', drop = False)
#validate(diffbind_md, schema="../schemas/diffbind.schema.yml")

# parse config files

samps = get_samples()
reads = get_reads()
marks = get_marks()
#mark_conditions = get_mark_conditions()

# base output directory from config
DATA_DIR = config["output_base_dir"].rstrip("/")

sample_noigg = [k for k in samps if config["IGG"] not in k]
marks_noigg = [m for m in marks if config["IGG"] not in m]

blacklist_file = config["BLACKLIST"].strip()

include: "rules/align.smk"
include: "rules/diff.smk"
include: "rules/peaks.smk"
include: "rules/postalign.smk"
include: "rules/preprocess.smk"
include: "rules/qc.smk"
localrules: frip_plot, fraglength_plot

#singularity: "library://gartician/miniconda-mamba/4.12.0:sha256.7302640e37d37af02dd48c812ddf9c540a7dfdbfc6420468923943651f795591"
# singularity: "/home/groups/MaxsonLab/software/singularity-containers/4.12.0_sha256.7302640e37d37af02dd48c812ddf9c540a7dfdbfc6420468923943651f795591.sif"

rule all:
    input:
        #expand(f"{DATA_DIR}/counts/{{mark}}_counts.tsv", mark=marks_noigg),
        #expand(f"{DATA_DIR}/counts/{{mark}}_consensus.bed", mark=marks_noigg),
        expand([f"{DATA_DIR}/Important_processed/Bam/{{sample}}.sorted.markd.bam",
                f"{DATA_DIR}/Important_processed/Bam/{{sample}}.sorted.markd.fraglen.tsv",
                f"{DATA_DIR}/Important_processed/Track/tracks/{{sample}}.bw",
                f"{DATA_DIR}/Report/preseq/lcextrap_{{sample}}.txt",
                ], sample=samps),
        # Peak calling outputs (gopeaks)
        #expand(f"{DATA_DIR}/callpeaks/{{sample}}_peaks.bed", sample=sample_noigg),
        # MACS2 outputs (broad for H3K27me3, narrow for others)
        get_macs2_outputs(DATA_DIR, config["IGG"]),
        # FRiP & fingerprint
        expand(f"{DATA_DIR}/Report/dtools/fingerprint_{{sample}}.tsv", sample=sample_noigg),
        expand(f"{DATA_DIR}/Report/plotEnrichment/frip_{{sample}}.tsv", sample=sample_noigg),

        #expand(f"{DATA_DIR}/callpeaks/{{sample}}_peaks_noBlacklist.bed", sample=samps) if os.path.isfile(blacklist_file) else [],

        f"{DATA_DIR}/Report/multiqc/multiqc_report.html",
        f"{DATA_DIR}/Report/multiqc/multiqc_data/multiqc_data.json",

        #expand([f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-rld-pca.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-vsd-pca.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-normcounts.csv",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-lognormcounts.csv",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-rld.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-vsd.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-vsd-dist.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-rld-dist.pdf",
        #f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-dds.rds",
        #f"{DATA_DIR}/homer/{{mark}}.done"], mark=marks_noigg),

        #expand(f"{DATA_DIR}/diffbind/{{mark}}/{{mark}}_DBAobj.rds", mark=marks_noigg),
        #expand(f"{DATA_DIR}/diffbind/{{mark}}/{{mark}}_pca.pdf", mark=marks_noigg),
        # quality control plots
        f"{DATA_DIR}/Report/fraglen.html",
        f"{DATA_DIR}/Report/plotEnrichment/frip.html",
        f"{DATA_DIR}/Report/peak_stat/peakcount.txt",
        # Genomic coverage report
        f"{DATA_DIR}/Report/coverage_report.tsv",
        
        #expand(f"{DATA_DIR}/mergebw/{{mark_condition}}.bw", mark_condition=mark_conditions),
        #expand(f"{DATA_DIR}/highConf/{{mark_condition}}.highConf.bed", mark_condition=mark_conditions),
        #f"{DATA_DIR}/custom_report/custom_report.html"



if config["USE_SINGULARITY"]:
    rule homer:
        input:
            f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-dds.rds"
        output:
            f"{DATA_DIR}/homer/{{mark}}.done"
        singularity:
            os.path.join(config["SINGULARITY_IMAGE_FOLDER"], "homer.sif")
        shell:
            f"OUTPUT_BASE_DIR=\"{DATA_DIR}\" bash src/homer.sh -m {{wildcards.mark}} -s 0 -p 8 -g {{config[FASTA]}}"
else:
    rule homer:
        input:
            f"{DATA_DIR}/deseq2/{{mark}}/{{mark}}-dds.rds"
        output:
            f"{DATA_DIR}/homer/{{mark}}.done"
        conda:
            "envs/homer.yml"
        shell:
            f"OUTPUT_BASE_DIR=\"{DATA_DIR}\" bash src/homer.sh -m {{wildcards.mark}} -s 1 -p 8 -g {{config[FASTA]}}"
# this rule submits HOMER runs to SLURM if -s = 1. A run is each unique contrast
# if running pipeline via containers, then don't allow homer script to spawn sbatch jobs because slurm can't be accessed inside container

